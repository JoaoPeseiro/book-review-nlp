{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad362984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from JSONL.GZ file\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "\n",
    "file_path = r\"C:\\Users\\megap\\Desktop\\Projeto\\Books.jsonl.gz\"\n",
    "\n",
    "# Load up to 100,000 lines from the gzipped JSONL file\n",
    "data = []\n",
    "with gzip.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 100_000:\n",
    "            break\n",
    "        try:\n",
    "            data.append(json.loads(line))\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head())\n",
    "print(df.columns)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f8807",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "distilled_student_sentiment_classifier = pipeline(\n",
    "    model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\",\n",
    "    return_all_scores=True\n",
    ")\n",
    "\n",
    "def get_predicted_label(text):\n",
    "    preds = distilled_student_sentiment_classifier(text[:512])[0]\n",
    "    return max(preds, key=lambda x: x[\"score\"])[\"label\"]\n",
    "\n",
    "df[\"predicted_sentiment\"] = df[\"text\"].progress_apply(get_predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a70a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_to_sentiment(score):\n",
    "    if score >= 4:\n",
    "        return \"positive\"\n",
    "    elif score == 3:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"negative\"\n",
    "\n",
    "df[\"sentiment\"] = df[\"rating\"].apply(rating_to_sentiment)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df[\"sentiment\"], df[\"predicted_sentiment\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f461185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "\n",
    "df[\"sentiment\"].value_counts()\n",
    "min_class_size = df[\"sentiment\"].value_counts().min()\n",
    "balanced_df = df.groupby(\"sentiment\").apply(lambda x: x.sample(min_class_size, random_state=42)).reset_index(drop=True)\n",
    "print(balanced_df[\"sentiment\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d3db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=) \n",
    "\n",
    "def cluster_texts_zero_shot(texts, max_texts_per_prompt=20):\n",
    "    categoria_por_texto = {}\n",
    "    for i in range(0, len(texts), max_texts_per_prompt):\n",
    "        batch = texts[i:i+max_texts_per_prompt]\n",
    "\n",
    "        prompt = \"\"\"\n",
    "Aqui est치 um exemplo de como agrupar textos em categorias:\n",
    "\n",
    "Categoria 1: Romance Hist칩rico  \n",
    "Textos: [1, 3]\n",
    "Categoria 2: Autoajuda  \n",
    "Textos: [2, 4]\n",
    "Categoria 3: Fic칞칚o Cient칤fica  \n",
    "Textos: [5]\n",
    "Categoria 4: Livros Infantis  \n",
    "Textos: [6]\n",
    "Categoria 5: Ensaios Acad칡micos  \n",
    "Textos: [7]\n",
    "\n",
    "Agora agrupa estes textos em **exatamente 5 categorias tem치ticas distintas**.  \n",
    "D치 a cada categoria um nome curto, claro e representativo. De seguida, indica os textos que pertencem a cada categoria.\n",
    "\n",
    "Textos:\n",
    "\"\"\"\n",
    "        for idx, text in enumerate(batch):\n",
    "            prompt += f\"{idx+1}. {text[:200]}...\\n\"\n",
    "\n",
    "        prompt += \"\"\"\n",
    "Responde no seguinte formato:\n",
    "Categoria 1: [nome]\n",
    "Textos: [n칰meros]\n",
    "Categoria 2: [nome]\n",
    "Textos: [n칰meros]\n",
    "Categoria 3: [nome]\n",
    "Textos: [n칰meros]\n",
    "Categoria 4: [nome]\n",
    "Textos: [n칰meros]\n",
    "Categoria 5: [nome]\n",
    "Textos: [n칰meros]\n",
    "\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.4\n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content\n",
    "        categoria_atual = None\n",
    "        for linha in answer.strip().split(\"\\n\"):\n",
    "            if linha.startswith(\"Categoria\"):\n",
    "                categoria_atual = linha.split(\":\")[1].strip()\n",
    "            elif linha.startswith(\"Textos\") and categoria_atual:\n",
    "                try:\n",
    "                    indices = eval(linha.split(\":\")[1].strip())\n",
    "                    for idx in indices:\n",
    "                        global_idx = i + idx - 1\n",
    "                        categoria_por_texto[global_idx] = categoria_atual\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    return categoria_por_texto\n",
    "\n",
    "categorias_map = cluster_texts_zero_shot(balanced_df[\"clean_text\"].tolist())\n",
    "balanced_df[\"categoria_original\"] = balanced_df.index.map(categorias_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7256287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalizar_texto(texto):\n",
    "    texto = str(texto).lower()\n",
    "    texto = unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    texto = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", texto)\n",
    "    texto = re.sub(r\"\\s+\", \" \", texto).strip()\n",
    "    return texto\n",
    "\n",
    "balanced_df = balanced_df.dropna(subset=[\"categoria_original\"]).reset_index(drop=True)\n",
    "balanced_df[\"categoria_limpa\"] = balanced_df[\"categoria_original\"].apply(normalizar_texto)\n",
    "\n",
    "# Reclustering categories with TF-IDF and KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cat_counts = balanced_df[\"categoria_limpa\"].value_counts()\n",
    "categorias_frequentes = cat_counts[cat_counts >= 5].index.tolist()\n",
    "balanced_df = balanced_df[balanced_df[\"categoria_limpa\"].isin(categorias_frequentes)]\n",
    "\n",
    "categories = balanced_df[\"categoria_limpa\"].unique().tolist()\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(categories)\n",
    "\n",
    "num_clusters = min(10, len(categories))\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "cat_freqs = balanced_df[\"categoria_limpa\"].value_counts().to_dict()\n",
    "category_map = {}\n",
    "for cluster_num in range(num_clusters):\n",
    "    cluster_cats = [cat for cat, label in zip(categories, labels) if label == cluster_num]\n",
    "    main_cat = max(cluster_cats, key=lambda c: cat_freqs.get(c, 0))\n",
    "    for cat in cluster_cats:\n",
    "        category_map[cat] = main_cat\n",
    "\n",
    "balanced_df[\"categoria_normalizada\"] = balanced_df[\"categoria_limpa\"].map(category_map).fillna(balanced_df[\"categoria_limpa\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0174b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def resumir_textos_openai(textos, categoria, max_texts=100):\n",
    "    joined_text = \" \".join(textos[:max_texts])[:4000]\n",
    "    prompt = f\"\"\"\n",
    "Faz um resumo curto e representativo dos seguintes coment치rios de clientes sobre livros da categoria \\\"{categoria}\\\".\n",
    "\n",
    "Coment치rios:\n",
    "{joined_text}\n",
    "\n",
    "Resumo:\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "        max_tokens=200\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "resumos_por_categoria = {}\n",
    "for cat in balanced_df[\"categoria_normalizada\"].unique():\n",
    "    textos_cat = balanced_df[balanced_df[\"categoria_normalizada\"] == cat][\"clean_text\"].tolist()\n",
    "    resumo = resumir_textos_openai(textos_cat, categoria=cat)\n",
    "    resumos_por_categoria[cat] = resumo\n",
    "    print(f\"\\n游댳 Summary for '{cat}':\\n{textwrap.fill(resumo, width=100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c035de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recomendar_livros_por_categoria(df, categoria, top_n=5):\n",
    "    subset = df[df[\"categoria_normalizada\"] == categoria]\n",
    "    if subset.empty:\n",
    "        print(\"Categoria n칚o encontrada.\")\n",
    "        return []\n",
    "    agrupado = subset.groupby(\"title\").agg({\"rating\": \"mean\", \"text\": \"first\", \"asin\": \"first\"}).reset_index()\n",
    "    top_livros = agrupado.sort_values(by=\"rating\", ascending=False).head(top_n)\n",
    "    return top_livros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20abde45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_top_livros_por_categoria(df, categoria, top_n=5, min_rating=4):\n",
    "    subset = df[(df[\"categoria_normalizada\"] == categoria) & (df[\"rating\"] >= min_rating)]\n",
    "    subset = subset.sort_values(by=\"rating\", ascending=False).head(top_n)\n",
    "\n",
    "    if subset.empty:\n",
    "        print(f\"Nenhum livro encontrado para a categoria '{categoria}' com rating >= {min_rating}.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n游닄 Top {top_n} livros da categoria: {categoria}\\n\")\n",
    "    for _, row in subset.iterrows():\n",
    "        print(f\"游닂 {row['title']} ({row['rating']:.1f} estrelas)\")\n",
    "        print(\"游닇 Exemplo de cr칤tica:\")\n",
    "        print(textwrap.fill(row[\"text\"], width=100))\n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "categoria_escolhida = \"romance\"\n",
    "mostrar_top_livros_por_categoria(balanced_df, categoria_escolhida, top_n=5)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
